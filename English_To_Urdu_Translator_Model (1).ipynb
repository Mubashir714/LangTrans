{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Installing the Require Dependencies\n",
        "Use the datasets, transformers and accelerate to get the data and perform operations on the model."
      ],
      "metadata": {
        "id": "qeb3pzzJekyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers accelerate\n"
      ],
      "metadata": {
        "id": "NC6Nh-0Vp7Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Load Dataset\n",
        "Utilize the Tatoeba dataset for English-to-Urdu translation."
      ],
      "metadata": {
        "id": "3RryckopfBam"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-WMpE1KpW5l"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "The Tatoeba dataset is a multilingual dataset, and we specifically choose English (en) to Urdu (ur) translations.\n",
        "It will load a train and test split by default."
      ],
      "metadata": {
        "id": "Qu5PIaQ0fnKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"ur\")"
      ],
      "metadata": {
        "id": "H_k5DRu1q6hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load Tokenizer and Model\n",
        "# Explanation:\n",
        " We use the pre-trained English-to-Urdu model from Hugging Face (`Helsinki-NLP/opus-mt-en-ur`).\n",
        "The tokenizer helps in preparing text for the model, converting words into numerical format."
      ],
      "metadata": {
        "id": "HTQ2kkk8gh_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Helsinki-NLP/opus-mt-en-ur\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Ik41Da7brGp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Freeze Most Layers for Efficient Training\n",
        "#Explanation:\n",
        "Freezing most layers reduces memory usage and speeds up training.\n",
        "Only the specified layers are fine-tuned, which is efficient on a CPU.\n"
      ],
      "metadata": {
        "id": "8coqZTc6g-5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False  # Freeze all layers\n",
        "\n",
        "# Unfreeze specific layers for fine-tuning\n",
        "trainable_layers = [\"encoder.layers\", \"decoder.layers\", \"lm_head\"]  # Unfreeze high-level layers\n",
        "for name, param in model.named_parameters():\n",
        "    if any(layer in name for layer in trainable_layers):\n",
        "        param.requires_grad = True\n",
        "        print(f\"Unfreezing layer: {name}\")  # Debugging to confirm layers are unfrozen\n"
      ],
      "metadata": {
        "id": "IlRgcp-Wro94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Preprocess the Data\n",
        "# Explanation:\n",
        "- `translation` contains both English (en) and Urdu (ur) translations.\n",
        "- Tokenization converts the text to input IDs.\n",
        "- Padding ensures all sequences in a batch have the same length, while truncation trims longer sequences.\n",
        "- `remove_columns` removes the original columns after preprocessing.\n"
      ],
      "metadata": {
        "id": "tfwyfip2hrXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if 'test' split exists, if not split manually\n",
        "if \"test\" not in dataset:\n",
        "    dataset = dataset[\"train\"].train_test_split(test_size=0.2)  # 80% train, 20% test\n",
        "\n",
        "# Preprocess the Data\n",
        "def preprocess_function(examples):\n",
        "    # Access the English and Urdu translations\n",
        "    inputs = [example[\"en\"] for example in examples[\"translation\"]]\n",
        "    targets = [example[\"ur\"] for example in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "processed_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\n"
      ],
      "metadata": {
        "id": "Wof9I5o1rq4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Define Training Arguments\n",
        "# Explanation:\n",
        "Training arguments define how the model is trained, including learning rate, batch size, and evaluation strategy.\n"
      ],
      "metadata": {
        "id": "gK_P4HhciEg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",  # Directory to save model checkpoints\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
        "    learning_rate=5e-4,  # Learning rate for optimization\n",
        "    per_device_train_batch_size=4,  # Batch size for training\n",
        "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    save_total_limit=2,  # Limit on the number of saved checkpoints\n",
        "    predict_with_generate=True,  # Enable text generation during evaluation\n",
        "    fp16=False,  # Disable mixed precision (only relevant for GPUs)\n",
        "    report_to=\"none\",  # Disable logging to third-party platforms\n",
        ")"
      ],
      "metadata": {
        "id": "I5KonMAescEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f31a4a-0eba-4129-a881-155e868134c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Initialize Trainer\n",
        "# Explanation:\n",
        "The Trainer class handles the training loop, evaluation, and saving of the model.\n"
      ],
      "metadata": {
        "id": "iBq6g5PxiLEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_datasets[\"train\"],\n",
        "    eval_dataset=processed_datasets[\"test\"],\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "L58AT5MDtOhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49e1565-7b1f-4292-d594-9ebff8b54c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-f60f479765cd>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Train the Model\n",
        "# Explanation:\n",
        "The `train` method fine-tunes the model using the specified dataset.\n",
        "After training, the model is saved in the specified output directory.\n"
      ],
      "metadata": {
        "id": "tSH27TJ-iZaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "OpaowUoIt5mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Save the Model\n",
        "# Explanation:\n",
        "Save the fine-tuned model to a directory for later use."
      ],
      "metadata": {
        "id": "rWVLTuEvixm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./fine_tuned_en_ur_model\")"
      ],
      "metadata": {
        "id": "svMGxZGnAjP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Test the Fine-Tuned Model\n"
      ],
      "metadata": {
        "id": "ClzlJqTIi_xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = model.generate(**inputs)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "CFA_gs97duYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(\"Hi. How may I help you Sir?\"))"
      ],
      "metadata": {
        "id": "OeapOlLKd393",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299e68f8-d617-4fb9-e059-52adfb6a890a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "برائے مہربانی کیا میں آپ کی مدد کروں؟\n"
          ]
        }
      ]
    }
  ]
}